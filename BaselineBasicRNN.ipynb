{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aafbfec-1f0e-47fe-b145-cfadaa8432a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT STATEMENTS\n",
    "import selenium\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # supress scikit 'future warnings'\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib         \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import norm, kurtosis\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "random.seed(42)\n",
    "#from basketball_reference_scraper import ask_matches\n",
    "from basketball_reference_scraper.teams import get_roster, get_team_stats, get_opp_stats, get_roster_stats, get_team_misc\n",
    "from basketball_reference_scraper.players import get_stats, get_game_logs, get_player_headshot, get_player_suffix, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025ec853-e6b2-4e4e-a655-d43b6a1d23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing and modification cell\n",
    "# This entire cell essentially modifies the data into a format\n",
    "# that is usable for the rest of the code\n",
    "# Takes data from scraper and puts it into necessary format\n",
    "\n",
    "import os\n",
    "# assign directory\n",
    "directory = 'Player_CSV'\n",
    " \n",
    "def true_to_one(input):\n",
    "    if input:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def home_to_one(input):\n",
    "    if input == \"HOME\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def win_to_one(input):\n",
    "    if input == \"WIN\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def convert_df(dataframe):\n",
    "    df = dataframe\n",
    "    df['active'] = df['active'].apply(true_to_one)\n",
    "    df['location'] = df['location'].apply(home_to_one)\n",
    "    df['outcome'] = df['outcome'].apply(win_to_one)\n",
    "    df = df.drop(columns=['date', 'team', 'opponent'])\n",
    "    return df\n",
    "\n",
    "X = []\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        player_df = pd.read_csv(f)\n",
    "        player_df = convert_df(player_df)\n",
    "        player_np = player_df.to_numpy()\n",
    "        player_np = np.transpose(player_np)\n",
    "        X.append(player_np)\n",
    "        #print(player_np)\n",
    "        \n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f6ffae-fc6c-4e77-9fdf-f26bdea8578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Tanh function -> Returns tanh(input)\n",
    "def tanh(vec):\n",
    "    vec = np.where(vec < -500, -500, vec)\n",
    "    vec = np.where(vec > 500, 500, vec)\n",
    "    ans = (np.exp(vec) - np.exp(-vec))/(np.exp(vec) + np.exp(-vec))\n",
    "    #print(vec)\n",
    "    #print(ans)\n",
    "    return ans\n",
    "\n",
    "# Normalizes entire given data set\n",
    "# Excludes non-existant data (length 0)\n",
    "def normalize_all_data(all_input_data):\n",
    "    return_data = []\n",
    "    \n",
    "    for data in all_input_data:\n",
    "        \n",
    "        if np.shape(data)[1] == 0:\n",
    "            continue\n",
    "        \n",
    "        mean = np.mean(data, axis = 1, keepdims = True)\n",
    "        std = np.std(data, axis = 1, keepdims = True)\n",
    "        \n",
    "        # Modify standard deviation to near 0 if it is 0 (edge case errors)\n",
    "        std = np.where(std == 0, 0.00001, std)\n",
    "        \n",
    "        new_data = (data - mean)/std\n",
    "        \n",
    "        return_data.append(new_data)\n",
    "        \n",
    "    return return_data\n",
    "\n",
    "# Calculates the softmax of an inputted vector\n",
    "def softmax(vec):\n",
    "    max_vals = np.reshape(np.amax(x, axis = 1), (np.shape(x)[0], 1))\n",
    "    mod_x = x - max_vals\n",
    "    \n",
    "    denominator = np.reshape(np.sum(np.exp(mod_x), axis = 1), (np.shape(x)[0], 1))\n",
    "    \n",
    "    return np.exp(mod_x)/denominator\n",
    "\n",
    "# Randomly initializes weights along a normal distribution multiplied by constant\n",
    "def init_weights(features):\n",
    "    params = {}\n",
    "    const = 1e-2\n",
    "    \n",
    "    params[\"W_aa\"] = np.random.randn(features, features) * const\n",
    "    params[\"W_ax\"] = np.random.randn(features, features) * const\n",
    "    params[\"W_y\"] = np.random.randn(features, features) * const\n",
    "    params[\"b_a\"] = np.random.randn(features, 1) * const\n",
    "    params[\"b_y\"] = np.random.randn(features, 1) * const\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Calculates and returns squared loss for vectors\n",
    "def avg_loss_func(predicted_y, actual_y):\n",
    "    diff = actual_y - predicted_y\n",
    "    \n",
    "    loss = 0.05 * np.sum(np.square(diff))\n",
    "    \n",
    "    num = np.shape(diff)[1]\n",
    "    \n",
    "    return loss/num\n",
    "\n",
    "# Given a vector of predictions and actual target\n",
    "# returns the average inaccuracy over the features being predicted\n",
    "def avg_acc_func(predicted, actual_y):\n",
    "    actual_y[actual_y == 0] = 1\n",
    "    diff = abs(predicted - actual_y)\n",
    "    perc_diff = diff/(actual_y)\n",
    "    \n",
    "    perc_diff = np.where(perc_diff > 1e2, 100, perc_diff)\n",
    "    perc_diff = np.where(perc_diff < -1e2, -100, perc_diff)\n",
    "    \n",
    "    num = np.shape(diff)[1]\n",
    "    \n",
    "    total_avg_acc = np.sum(perc_diff)/num\n",
    "    \n",
    "    # Return the average inaccuracy\n",
    "    return total_avg_acc\n",
    "\n",
    "# Forward propagation for RNN model\n",
    "# Takes in input data and set of parameters\n",
    "# Returs predictions, targets, caches, loss, and accuracy\n",
    "def rnn_forward_prop(input_data, params):\n",
    "    \n",
    "    # NUmber of features -> At current stage is 20\n",
    "    features = np.shape(input_data)[0]\n",
    "    examples = np.shape(input_data)[1]\n",
    "    \n",
    "    actual_y = input_data[:]\n",
    "    #print(np.shape(actual_y))\n",
    "    actual_y_row = np.shape(actual_y)[0]\n",
    "    actual_y_col = np.shape(actual_y)[1]\n",
    "    \n",
    "    # Label each parameter for convenience\n",
    "    W_aa = params[\"W_aa\"]\n",
    "    W_ax = params[\"W_ax\"]\n",
    "    b_a = params[\"b_a\"]\n",
    "    W_y = params[\"W_y\"]\n",
    "    b_y = params[\"b_y\"]\n",
    "        \n",
    "    a_last = np.random.randn(features, 1) * 0.01\n",
    "    \n",
    "    cache_a = [a_last]\n",
    "    cache_x = []\n",
    "    \n",
    "    predictions = np.array([])\n",
    "    \n",
    "    # Copmute RNN style forward prop on all portions of the sequence\n",
    "    for example in range(examples):\n",
    "        #print(a_last)\n",
    "        x_curr = input_data[:, 0]\n",
    "        x_curr = np.reshape(x_curr, (features, 1))\n",
    "        \n",
    "        z_a_new = np.dot(W_aa, a_last) + np.dot(W_ax, x_curr) + b_a\n",
    "        \n",
    "        #print(np.shape(z_a_new))\n",
    "        \n",
    "        a_new = tanh(z_a_new)\n",
    "        y_new = np.dot(W_y, a_new) + b_y\n",
    "        \n",
    "        predictions = np.append(predictions, y_new)\n",
    "        \n",
    "        a_last = np.copy(a_new)\n",
    "        \n",
    "        # Add last a<i> value to cache along with the inputted x from sequence\n",
    "        cache_a.append(a_last)\n",
    "        cache_x.append(x_curr)\n",
    "        \n",
    "    predictions = np.reshape(predictions, (actual_y_row, actual_y_col))\n",
    "    \n",
    "    loss = avg_loss_func(predictions, actual_y)\n",
    "    \n",
    "    avg_acc = avg_acc_func(predictions, actual_y)\n",
    "    \n",
    "    # Return all the components we want to return and store and use later\n",
    "    return predictions, actual_y, cache_a, cache_x, loss, avg_acc\n",
    "\n",
    "# This is the backward prop function that takes in cached values and parameters\n",
    "# and then returns the gradients for each parameter in a dictionary\n",
    "def rnn_backward_prop(predictions, actual_y_vals, params, cache_a, cache_x):\n",
    "    grads = {}\n",
    "    \n",
    "    num = np.shape(predictions)[1]\n",
    "    \n",
    "    for param, np_arr in params.items():\n",
    "        dim = np.shape(np_arr)\n",
    "        grads[param] = np.zeros(dim)\n",
    "    \n",
    "    da_future = np.zeros((np.shape(predictions[:, 0])[0], 1))\n",
    "    \n",
    "    # Go through every single layer of the RNN and then modify calculate portions\n",
    "    # step by step and then modify the overall gradient value since the gradient\n",
    "    # of each parameter is going to take in more layers over time (as we go back)\n",
    "    for i in range(num, 0, -1):\n",
    "        y_actual = actual_y_vals[:, i-1]\n",
    "        y_hat = predictions[:, i-1]\n",
    "        x_i = cache_x[i-1]\n",
    "        a_i = cache_a[i]\n",
    "        a_i_last = cache_a[i-1]\n",
    "        \n",
    "        dy = y_hat - y_actual\n",
    "        dy = np.reshape(dy, (np.shape(dy)[0], 1))\n",
    "        #print(dy)\n",
    "        da = np.dot(params[\"W_y\"].T, dy)\n",
    "        dz = da * (1 + a_i) * (1 - a_i)\n",
    "        dz_future = da_future * (1 + a_i) * (1 - a_i)\n",
    "         \n",
    "        d_by = np.sum(dy, axis = 1, keepdims = True)\n",
    "        d_Wy = np.dot(dy, a_i.T)\n",
    "        \n",
    "        d_ba = np.sum(dz, axis = 1, keepdims = True)\n",
    "        d_Waa = np.dot(dz + dz_future, a_i_last.T)\n",
    "        d_Wax = np.dot(dz + dz_future, x_i.T)\n",
    "        \n",
    "        da_future = np.copy(da)\n",
    "        \n",
    "        grads[\"b_y\"] += d_by\n",
    "        grads[\"W_y\"] += d_Wy\n",
    "        grads[\"b_a\"] += d_ba\n",
    "        grads[\"W_aa\"] += d_Waa\n",
    "        grads[\"W_ax\"] += d_Wax\n",
    "    \n",
    "    # Return the updated gradients\n",
    "    return grads\n",
    "\n",
    "# Get the gradients and update the parameters by the given step size amount\n",
    "def update_params(predictions, actual_y_vals, params, cache_a, cache_x, step_size = 1e-3):\n",
    "    grads = rnn_backward_prop(predictions, actual_y_vals, params, cache_a, cache_x)\n",
    "    #print(grads)\n",
    "    for param in params.keys():\n",
    "        params[param] = params[param] - step_size * grads[param]\n",
    "    \n",
    "    #Return the modified parameters (dict)\n",
    "    return params\n",
    "    \n",
    "# Splits the entire given data into a train and test array ->\n",
    "# Perc_train designates what portion should be given to training\n",
    "# while the rest is test\n",
    "def train_test_split(all_input_data, perc_train = 0.8):\n",
    "    num_examples = len(all_input_data)\n",
    "    \n",
    "    train_examples = int((perc_train * num_examples) // 1)\n",
    "    print(train_examples)\n",
    "    test_examples = num_examples - train_examples\n",
    "    \n",
    "    all_train_data = all_input_data[:train_examples]\n",
    "    all_test_data = all_input_data[train_examples:]\n",
    "    \n",
    "    return all_train_data, all_test_data\n",
    "\n",
    "# This function is the full RNN baseline model with given input data\n",
    "def rnn(all_input_data):\n",
    "    # Split the data into train and test (0.8 train)\n",
    "    all_train_data, all_test_data = train_test_split(all_input_data, 0.8)\n",
    "    \n",
    "    # Get the number of features and initialize parameters\n",
    "    features = np.shape(all_train_data[0])[0]\n",
    "    params = init_weights(features)\n",
    "    \n",
    "    loss_arr = np.array([])\n",
    "    avg_acc_arr = np.array([])\n",
    "    \n",
    "    discount_games = 0\n",
    "    \n",
    "    # Copmute 5 epochs of essentially stochastically going through entire training\n",
    "    # data set\n",
    "    for epoch in range(5):\n",
    "        # Go through each player & season games and train on it\n",
    "        for example_num, train_example in enumerate(all_train_data):\n",
    "            games = np.shape(train_example)[1]\n",
    "\n",
    "            if games == 0:\n",
    "                discount_games += 1\n",
    "                continue\n",
    "            \n",
    "            # Compute forward prop and then update the parameters\n",
    "            predictions, actual_y, cache_a, cache_x, loss, avg_acc = rnn_forward_prop(train_example, params)\n",
    "            params = update_params(predictions, actual_y, params, cache_a, cache_x)\n",
    "            \n",
    "            # Keep track of the loss and average accuracy during this iteration\n",
    "            loss_arr = np.append(loss_arr, loss)\n",
    "            avg_acc_arr = np.append(avg_acc_arr, avg_acc)\n",
    "    \n",
    "    \n",
    "    x_amount = len(all_train_data) - discount_games\n",
    "    x_axis = np.arange(len(loss_arr))\n",
    "    \n",
    "    x_axis_new = np.linspace(x_axis.min(), x_axis.max(), 200) \n",
    "\n",
    "    #define spline\n",
    "    spl = make_interp_spline(x_axis, loss_arr, k=1)\n",
    "    loss_smooth = spl(x_axis_new)\n",
    "\n",
    "    # Plot the loss over the training examples with smoothing\n",
    "    plt.clf()\n",
    "    plt.plot(x_axis_new, loss_smooth)\n",
    "    plt.title(\"Average loss over training examples\")\n",
    "    plt.show()\n",
    "    \n",
    "    #plt.clf()\n",
    "    #plt.plot(x_axis, avg_acc_arr)\n",
    "    #plt.title(\"Average Inaccuracy over training examples\")\n",
    "    #plt.show()\n",
    "    \n",
    "    test_loss_arr = np.array([])\n",
    "    test_avg_acc = np.array([])\n",
    "    \n",
    "    # Go through the testing examples and compute only forward prop\n",
    "    # Add in the testing loss and accuracy values for evaluation\n",
    "    for example_num, test_example in enumerate(all_test_data):\n",
    "        games = np.shape(test_example)[1]\n",
    "        \n",
    "        # Discard non-existant data with length 0\n",
    "        if games == 0:\n",
    "            discount_games += 1\n",
    "            continue\n",
    "\n",
    "        predictions, actual_y, cache_a, cache_x, loss, avg_acc = rnn_forward_prop(test_example, params)\n",
    "        #params = update_params(predictions, actual_y, params, cache_a, cache_x)\n",
    "        #if example_num < 100:\n",
    "        #    print(loss)\n",
    "\n",
    "        test_loss_arr = np.append(test_loss_arr, loss)\n",
    "        test_avg_acc = np.append(test_avg_acc, avg_acc)\n",
    "    \n",
    "    print(\"Average Loss (Test): \" + str(np.mean(test_loss_arr)) + \"\")\n",
    "    print(\"Average Inaccuracy (Test): \" + str(np.mean(test_avg_acc)) + \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b1f9a3-a980-460e-8465-3a8a60ee74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = normalize_all_data(X)\n",
    "#print(test_input)\n",
    "rnn(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49990a80-e743-436d-a462-ddc62213b046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
