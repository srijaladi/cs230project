{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSRuV6-3JW5Z"
   },
   "source": [
    "Prepare the Data:\n",
    "1. Scrape basketball-reference for player game logs\n",
    "2. Transform the data into (m, 10, 20) game sequences, each game with 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDzRsmd6ngg3"
   },
   "outputs": [],
   "source": [
    "#Install the first basketball-reference scraper\n",
    "pip install basketball-reference-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZtW2cd5e4Cw"
   },
   "outputs": [],
   "source": [
    "#Install the second basketball-reference scraper\n",
    "pip install basketball_reference_web_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKs5NblHnFiY"
   },
   "outputs": [],
   "source": [
    "#IMPORT STATEMENTS\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # supress scikit 'future warnings'\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib         \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import norm, kurtosis\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import Callable, Dict, List, Tuple, TypeVar\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "#from basketball_reference_scraper import ask_matches\n",
    "from basketball_reference_scraper.teams import get_roster, get_team_stats, get_opp_stats, get_roster_stats, get_team_misc\n",
    "from basketball_reference_scraper.players import get_stats, get_game_logs, get_player_headshot, get_player_suffix, lookup\n",
    "import datetime\n",
    "from basketball_reference_web_scraper import client\n",
    "from basketball_reference_web_scraper.data import Location \n",
    "from basketball_reference_web_scraper.data import Outcome \n",
    "from basketball_reference_web_scraper.data import OutputType\n",
    "import pprint \n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBUz3SKRpAR8"
   },
   "outputs": [],
   "source": [
    "#Create a set of all basketball players (NBA) going back about 20 years\n",
    "entire_player_list = set()\n",
    "teams = [\n",
    "         \"ATL\", \"NJN\", \"BOS\", \"CHA\", \"CHI\", \"CLE\", \"DAL\", \"DEN\", \"DET\", \"GSW\", \n",
    "         \"HOU\", \"IND\", \"LAC\", \"LAL\", \"MEM\", \"MIA\", \"MIL\", \"MIN\", \"NOP\", \"NYK\",\n",
    "         \"OKC\", \"ORL\", \"PHI\", \"PHO\", \"POR\", \"SAC\", \"SAS\", \"TOR\", \"UTA\", \"WAS\", \"CHO\", \"BRK\"\n",
    "]\n",
    "for team in teams:\n",
    "  for year in range(2000,2022):\n",
    "    try:\n",
    "      for player in get_roster(team, year)['PLAYER']:\n",
    "        entire_player_list.add(player)\n",
    "    except:\n",
    "      print(team, year)\n",
    "\n",
    "print(len(entire_player_list), entire_player_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKTu0Yri_TnY"
   },
   "outputs": [],
   "source": [
    "#A method to create a player's nba-reference id from their name for use in\n",
    "#getting that player's game log csv\n",
    "def create_id(player_name):\n",
    "  player_names = player_name.strip(\"-_.,\\'~`\").split(\" \")\n",
    "  first = player_names[0]\n",
    "  last = player_names[1]\n",
    "  player_id = last.lower().strip()[0:min(5,len(last))] + first.lower()[0:min(2,len(first))] + \"01\"\n",
    "  return player_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtCLGCG_Csnl"
   },
   "outputs": [],
   "source": [
    "#A method to save each player's season game logs to a csv in Google Drive for \n",
    "#easy future use.\n",
    "def output_file(player_id, player_name, year):\n",
    "  try:\n",
    "    client.regular_season_player_box_scores(player_identifier=player_id, \n",
    "                                    season_end_year=year, \n",
    "                                    output_type=OutputType.CSV, \n",
    "                                    output_file_path=\"/content/gdrive/My Drive/freshman year/CS230/Notebooks/Final Project/Player CSVs 3/%d_%d_%s_%s.csv\" %(year-1, year, player_id, player_name))\n",
    "    print(\"succesful\", player_name, year)\n",
    "  except:\n",
    "    print(\"unsuccesful\", player_name, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5YWqEq38vaE"
   },
   "outputs": [],
   "source": [
    "#Save all possible season game logs to google drive using the list of all\n",
    "#active players, the possible seasons they could've played in, and their\n",
    "#player id.\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "exceptions = []\n",
    "#output player data\n",
    "for player in entire_player_list:\n",
    "  try:\n",
    "    player_id = create_id(player)\n",
    "    for year in range(2000, 2022):\n",
    "        output_file(player_id, player, year)\n",
    "  except:\n",
    "    print(\"unsuccesful 2 \" + player)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-wR1vIfnZK-"
   },
   "outputs": [],
   "source": [
    "#Mount google drive so that files stored in Google Drive\n",
    "#can readily be accessed within this notebook.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3i3hqW3xCKd2"
   },
   "outputs": [],
   "source": [
    "#Methods used for transforming different words in the game CSVs into \n",
    "#number values\n",
    "\n",
    "#Transform True to 1 and False to 0\n",
    "def true_to_one(input):\n",
    "  if input:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "#Transform \"HOME\" to 1 and \"AWAY\" to 0\n",
    "def home_to_one(input):\n",
    "  if input == \"HOME\":\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "#Transform \"WIN\" to 1 and \"LOST\" to 0\n",
    "def win_to_one(input):\n",
    "  if input == \"WIN\":\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "#Using the transformation columns, convert each game log into solely numerical\n",
    "#values used for the supervised learning task\n",
    "def convert_df(dataframe):\n",
    "  df = dataframe\n",
    "  df['active'] = df['active'].apply(true_to_one)\n",
    "  df['location'] = df['location'].apply(home_to_one)\n",
    "  df['outcome'] = df['outcome'].apply(win_to_one)\n",
    "  df = df.drop(columns=['date', 'team', 'opponent'])\n",
    "  return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUULZKI-Of91",
    "outputId": "71b1db09-0a57-418a-bfc0-7fa1e13dae7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/freshman year/CS230/Notebooks/Final Project/Player CSVs 3\n",
      "1\n",
      "1001\n",
      "2001\n",
      "3001\n",
      "4001\n",
      "5001\n",
      "6001\n",
      "7001\n",
      "8001\n",
      "9001\n",
      "/content/gdrive/My Drive/freshman year/CS230/Notebooks/Final Project\n"
     ]
    }
   ],
   "source": [
    "#Navigate to the appropriate directory to access all player game logs\n",
    "%cd /content/gdrive/My Drive/freshman year/CS230/Notebooks/Final Project/Player CSVs 3/\n",
    "file_names = sp.getoutput('ls')\n",
    "file_names = file_names.split('\\n')\n",
    "\n",
    "#Iterate through each player season log and transform that season into a \n",
    "#numpy array and add that array to a list of all players' seasons\n",
    "i = 0\n",
    "X = []\n",
    "\n",
    "for file_name in file_names:\n",
    "  player_df = pd.read_csv(\"/content/gdrive/My Drive/freshman year/CS230/Notebooks/Final Project/Player CSVs 3/\" + file_name)\n",
    "  player_df = convert_df(player_df)\n",
    "  player_np = player_df.to_numpy()\n",
    "  player_np = np.transpose(player_np)\n",
    "  X.append(player_np)\n",
    "  if (i % 1000) == 1:\n",
    "    print(i)\n",
    "  i += 1\n",
    "\n",
    "#navigate out to the final project folder for saving models later\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATIkb3qHezJ4",
    "outputId": "27d17bc8-51f2-47b0-d5f0-edcd2c9be14f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9226\n"
     ]
    }
   ],
   "source": [
    "#Create a sliding window for each player season game logs that takes\n",
    "#series of 10 games. By this method, we create around 400,000 usable \n",
    "#datapoints.\n",
    "sequence_len = 10\n",
    "inputX = np.zeros((400001, sequence_len, 20))\n",
    "#j is used for keeping track of where in all datapoints (ie how many valid \n",
    "#series have been created) the loop is at\n",
    "j = 0\n",
    "\n",
    "for xSeason in X:\n",
    "  #i is used to index through each player season, ie how many different windows\n",
    "  #can be created in lengths of 10 to create as many possible datapoints\n",
    "  #from a season\n",
    "  i = 0\n",
    "  while (i + sequence_len <= xSeason.shape[1]):\n",
    "    inputX[j, :, :] = xSeason[:, i:i + sequence_len].T\n",
    "    i += 1\n",
    "    j += 1\n",
    "    if j >= 400000:\n",
    "      break\n",
    "  \n",
    "#only keep the valid datapoints\n",
    "inputX = inputX[0:396700, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWAD3j45KoLw"
   },
   "source": [
    "Begin preparing the data for use in supervised learning by splitting it into a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTSqeD63Fz77"
   },
   "outputs": [],
   "source": [
    "#IMPORT STATEMENTS\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # supress scikit 'future warnings'\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib         \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import norm, kurtosis\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import Callable, Dict, List, Tuple, TypeVar\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import pprint \n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import subprocess as sp\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Xk8_Mo-FNlZ"
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset a split it into train and test\n",
    "# Input: (None, 9, 20)\n",
    "# Output: (None, 1, 12)-only keep the 12 relevant stats for fantasy score from \n",
    "# the 10th game.\n",
    "\n",
    "X, y = inputX[:, 0:9, :], inputX[:, 9, [1, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7mquOZC5Czc"
   },
   "outputs": [],
   "source": [
    "#create normalization layer and adapt to (ie compute mean and variance using)\n",
    "#the training X data.\n",
    "normalize_layer = tf.keras.layers.Normalization(axis = -1)\n",
    "normalize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwDftYH59byz"
   },
   "outputs": [],
   "source": [
    "#Declare 2 utilitarian methods for use of evaluating and training our model\n",
    "\n",
    "#transform a (m, 12) array of stats into a (m) array of fantasy scores\n",
    "#computed from those stats\n",
    "def scores_from_stats(stats):\n",
    "  fantasy_score_arr = np.array([1,2,-1,1,1,-1,1,1,2,4,4,-2])\n",
    "  score = np.zeros((stats.shape[0]))\n",
    "  for i in range(stats.shape[0]):\n",
    "    score[i] = np.dot(stats[i], fantasy_score_arr)\n",
    "  score = np.squeeze(score)\n",
    "  return score\n",
    "\n",
    "#compute stat-specific losses, ie for each of the 12 predicted stats, compute \n",
    "#a loss\n",
    "def stat_specific_loss(y_true, y_hat):\n",
    "  #shape of y_true and y_hat is (m, 12), calculates RMS\n",
    "  loss = np.sqrt((1/y_true.shape[0]) * np.sum((y_true - y_hat) ** 2, axis = 0))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3hqU7l85SBL"
   },
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuvU4CNzImRF"
   },
   "outputs": [],
   "source": [
    "#Define an LSTM model which predicts stats\n",
    "def LSTM_stats_model(Tx, num_stats_out, end_to_end = False):\n",
    "  #create the input, which is a (9, 20) array (9 games with 20 features)\n",
    "  input = tf.keras.Input(shape=(Tx-1, 20)) \n",
    "\n",
    "  #normalize the input array on the feature axis \n",
    "  mid = normalize_layer(input)\n",
    "\n",
    "  #create three LSTM layers which are many-to-many\n",
    "  mid = tf.keras.layers.LSTM(128, dropout = 0.2, return_sequences = True)(mid)\n",
    "  mid = tf.keras.layers.LSTM(128, dropout = 0.2, return_sequences = True)(mid)\n",
    "  mid = tf.keras.layers.LSTM(64, dropout = 0.2, return_sequences = True)(mid)\n",
    "\n",
    "  #create a final LSTM layer which is many-to-one and outputs a single (64) array\n",
    "  mid = tf.keras.layers.LSTM(64, dropout = 0.2)(mid)\n",
    "\n",
    "  #pass that final output through 3 dense layers to output a prediction for the\n",
    "  #stats of the 12th game.\n",
    "  mid = tf.keras.layers.Dense(32, activation=\"relu\")(mid)\n",
    "  #mid = tf.keras.layers.Dropout(0.15)(mid)\n",
    "  mid = tf.keras.layers.Dense(16, activation=\"relu\")(mid)\n",
    "  out = tf.keras.layers.Dense(num_stats_out, activation=\"relu\")(mid)\n",
    "\n",
    "  #optionally add a final layer for directly predicting fantasy scores\n",
    "  if end_to_end:\n",
    "    out = tf.keras.layers.Dense(1, activation=\"linear\")(out)\n",
    "\n",
    "  #declare the model\n",
    "  model_stats_out = tf.keras.Model(inputs=input, outputs=out)\n",
    "\n",
    "  #create an optimizer for the model\n",
    "  opt = tf.keras.optimizers.Adam(lr=0.02, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "  #compile the model\n",
    "  model_stats_out.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
    "  return model_stats_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skOBxxXum0Ei",
    "outputId": "f2f37f59-a69e-48e0-9b98-f9030a3a64f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 9, 20)]           0         \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 9, 20)            41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 9, 128)            76288     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 9, 128)            131584    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 9, 64)             49408     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 12)                204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 293,157\n",
      "Trainable params: 293,116\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create an LSTM stats-out model (ie a model which predicts the 12 stats of the\n",
    "#final game).\n",
    "stats_out_model = LSTM_stats_model(10, 12)\n",
    "stats_out_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3JdGMTovAWA",
    "outputId": "f7664fcf-4e46-4728-a5ac-c6effe443cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "5889/5889 [==============================] - 70s 10ms/step - loss: 1.7151 - accuracy: 0.6256\n",
      "Epoch 2/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6759 - accuracy: 0.6276\n",
      "Epoch 3/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6663 - accuracy: 0.6186\n",
      "Epoch 4/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6385 - accuracy: 0.6245\n",
      "Epoch 5/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6285 - accuracy: 0.6252\n",
      "Epoch 6/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6257 - accuracy: 0.6240\n",
      "Epoch 7/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6245 - accuracy: 0.6239\n",
      "Epoch 8/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6234 - accuracy: 0.6244\n",
      "Epoch 9/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6223 - accuracy: 0.6231\n",
      "Epoch 10/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6220 - accuracy: 0.6232\n",
      "Epoch 11/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6212 - accuracy: 0.6228\n",
      "Epoch 12/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6207 - accuracy: 0.6224\n",
      "Epoch 13/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6201 - accuracy: 0.6218\n",
      "Epoch 14/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6200 - accuracy: 0.6212\n",
      "Epoch 15/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6194 - accuracy: 0.6207\n",
      "Epoch 16/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6189 - accuracy: 0.6206\n",
      "Epoch 17/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6185 - accuracy: 0.6202\n",
      "Epoch 18/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6180 - accuracy: 0.6200\n",
      "Epoch 19/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6173 - accuracy: 0.6200\n",
      "Epoch 20/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6172 - accuracy: 0.6199\n",
      "Epoch 21/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6172 - accuracy: 0.6197\n",
      "Epoch 22/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6165 - accuracy: 0.6197\n",
      "Epoch 23/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6163 - accuracy: 0.6182\n",
      "Epoch 24/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6161 - accuracy: 0.6181\n",
      "Epoch 25/90\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.6158 - accuracy: 0.6173\n",
      "Epoch 26/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6155 - accuracy: 0.6181\n",
      "Epoch 27/90\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.6153 - accuracy: 0.6174\n",
      "Epoch 28/90\n",
      "5889/5889 [==============================] - 66s 11ms/step - loss: 1.6153 - accuracy: 0.6174\n",
      "Epoch 29/90\n",
      "5889/5889 [==============================] - 65s 11ms/step - loss: 1.6150 - accuracy: 0.6169\n",
      "Epoch 30/90\n",
      "5889/5889 [==============================] - 65s 11ms/step - loss: 1.6149 - accuracy: 0.6170\n",
      "Epoch 31/90\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.6141 - accuracy: 0.6173\n",
      "Epoch 32/90\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.6141 - accuracy: 0.6168\n",
      "Epoch 33/90\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.6142 - accuracy: 0.6166\n",
      "Epoch 34/90\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.6142 - accuracy: 0.6165\n",
      "Epoch 35/90\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.6134 - accuracy: 0.6164\n",
      "Epoch 36/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6134 - accuracy: 0.6159\n",
      "Epoch 37/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6134 - accuracy: 0.6158\n",
      "Epoch 38/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6134 - accuracy: 0.6162\n",
      "Epoch 39/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6130 - accuracy: 0.6161\n",
      "Epoch 40/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6127 - accuracy: 0.6159\n",
      "Epoch 41/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6126 - accuracy: 0.6162\n",
      "Epoch 42/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6125 - accuracy: 0.6160\n",
      "Epoch 43/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6122 - accuracy: 0.6163\n",
      "Epoch 44/90\n",
      "5889/5889 [==============================] - 56s 9ms/step - loss: 1.6123 - accuracy: 0.6154\n",
      "Epoch 45/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6124 - accuracy: 0.6159\n",
      "Epoch 46/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6124 - accuracy: 0.6161\n",
      "Epoch 47/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6121 - accuracy: 0.6147\n",
      "Epoch 48/90\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.6117 - accuracy: 0.6157\n",
      "Epoch 49/90\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.6119 - accuracy: 0.6151\n",
      "Epoch 50/90\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.6117 - accuracy: 0.6155\n",
      "Epoch 51/90\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.6117 - accuracy: 0.6154\n",
      "Epoch 52/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6115 - accuracy: 0.6158\n",
      "Epoch 53/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6113 - accuracy: 0.6160\n",
      "Epoch 54/90\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.6113 - accuracy: 0.6160\n",
      "Epoch 55/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6112 - accuracy: 0.6157\n",
      "Epoch 56/90\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.6114 - accuracy: 0.6153\n",
      "Epoch 57/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6112 - accuracy: 0.6158\n",
      "Epoch 58/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6111 - accuracy: 0.6156\n",
      "Epoch 59/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6110 - accuracy: 0.6162\n",
      "Epoch 60/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6106 - accuracy: 0.6154\n",
      "Epoch 61/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6112 - accuracy: 0.6151\n",
      "Epoch 62/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6108 - accuracy: 0.6157\n",
      "Epoch 63/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6105 - accuracy: 0.6160\n",
      "Epoch 64/90\n",
      "5889/5889 [==============================] - 56s 9ms/step - loss: 1.6105 - accuracy: 0.6155\n",
      "Epoch 65/90\n",
      "5889/5889 [==============================] - 56s 9ms/step - loss: 1.6103 - accuracy: 0.6155\n",
      "Epoch 66/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6103 - accuracy: 0.6161\n",
      "Epoch 67/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6102 - accuracy: 0.6158\n",
      "Epoch 68/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6101 - accuracy: 0.6159\n",
      "Epoch 69/90\n",
      "5889/5889 [==============================] - 55s 9ms/step - loss: 1.6104 - accuracy: 0.6161\n",
      "Epoch 70/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6101 - accuracy: 0.6157\n",
      "Epoch 71/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6100 - accuracy: 0.6153\n",
      "Epoch 72/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6099 - accuracy: 0.6156\n",
      "Epoch 73/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6103 - accuracy: 0.6159\n",
      "Epoch 74/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6099 - accuracy: 0.6150\n",
      "Epoch 75/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6097 - accuracy: 0.6160\n",
      "Epoch 76/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6097 - accuracy: 0.6164\n",
      "Epoch 77/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6099 - accuracy: 0.6158\n",
      "Epoch 78/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6097 - accuracy: 0.6149\n",
      "Epoch 79/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6096 - accuracy: 0.6157\n",
      "Epoch 80/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6093 - accuracy: 0.6156\n",
      "Epoch 81/90\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.6093 - accuracy: 0.6162\n",
      "Epoch 82/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6092 - accuracy: 0.6157\n",
      "Epoch 83/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6090 - accuracy: 0.6157\n",
      "Epoch 84/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6090 - accuracy: 0.6156\n",
      "Epoch 85/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6088 - accuracy: 0.6155\n",
      "Epoch 86/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6092 - accuracy: 0.6153\n",
      "Epoch 87/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6091 - accuracy: 0.6159\n",
      "Epoch 88/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6094 - accuracy: 0.6158\n",
      "Epoch 89/90\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.6089 - accuracy: 0.6161\n",
      "Epoch 90/90\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.6089 - accuracy: 0.6163\n"
     ]
    }
   ],
   "source": [
    "#Train the stats-out LSTM model\n",
    "with tf.device('/device:GPU:0'):\n",
    "  history = stats_out_model.fit(X_train, y_train, epochs=90, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "VT5zoTDTSk9A",
    "outputId": "b2166282-6d4d-4b4b-f458-19945e27f0c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss of the model during training\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAJbQBFtzTRk"
   },
   "outputs": [],
   "source": [
    "#Save and Evaluate LSTM stats out model\n",
    "stats_out_model.save(\"stats_out_model_05_27\")\n",
    "stats_out_model.evaluate(X_test, y_test)\n",
    "\n",
    "y_pred = stats_out_model.predict(X_test)\n",
    "loss = stat_specific_loss(y_test, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwfigFHn2cXb"
   },
   "outputs": [],
   "source": [
    "#Evaluate stats out model in terms of fantasy points\n",
    "\n",
    "y_pred = stats_out_model.predict(X_test)\n",
    "y_pred_scores = scores_from_stats(y_pred)\n",
    "y_test_scores = scores_from_stats(y_test)\n",
    "loss = tf.keras.losses.mean_squared_error(y_test_scores, y_pred_scores)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubzB9xK0ysjM",
    "outputId": "f20c2c81-e80a-439f-be00-b5831e11e3f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 8s 5ms/step - loss: 6.3425 - accuracy: 0.6491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.3425116539001465, 0.6491051316261292]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and evaluate model\n",
    "loaded_model = tf.keras.models.load_model(\"stats_out_model_05_27\")\n",
    "loaded_model.evaluate(X_test, y_test)\n",
    "\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "print(y_pred)\n",
    "stat_loss = stat_specific_loss(y_test, y_pred)\n",
    "y_pred_scores = scores_from_stats(y_pred)\n",
    "y_test_scores = scores_from_stats(y_test)\n",
    "loss = tf.keras.losses.mean_squared_error(y_test_scores, y_pred_scores)\n",
    "print(stat_loss)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itPRIffocMU8",
    "outputId": "2c2bca8e-48bd-450d-c3d4-91b77750dd37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 9, 20)]           0         \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 9, 20)            41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 9, 128)            76288     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 9, 128)            131584    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 9, 64)             49408     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 12)                204       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 293,170\n",
      "Trainable params: 293,129\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n",
      "[18. 35. 14. ...  5.  4. 16.]\n",
      "Epoch 1/70\n",
      "5889/5889 [==============================] - 66s 10ms/step - loss: 9.6032 - accuracy: 0.0206\n",
      "Epoch 2/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.4376 - accuracy: 0.0206\n",
      "Epoch 3/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.4211 - accuracy: 0.0206\n",
      "Epoch 4/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.4111 - accuracy: 0.0206\n",
      "Epoch 5/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.4020 - accuracy: 0.0206\n",
      "Epoch 6/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.4006 - accuracy: 0.0206\n",
      "Epoch 7/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3960 - accuracy: 0.0206\n",
      "Epoch 8/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3952 - accuracy: 0.0206\n",
      "Epoch 9/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3929 - accuracy: 0.0206\n",
      "Epoch 10/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3939 - accuracy: 0.0206\n",
      "Epoch 11/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3905 - accuracy: 0.0206\n",
      "Epoch 12/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3873 - accuracy: 0.0206\n",
      "Epoch 13/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3855 - accuracy: 0.0206\n",
      "Epoch 14/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3845 - accuracy: 0.0206\n",
      "Epoch 15/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3836 - accuracy: 0.0206\n",
      "Epoch 16/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3810 - accuracy: 0.0206\n",
      "Epoch 17/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3799 - accuracy: 0.0206\n",
      "Epoch 18/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3797 - accuracy: 0.0206\n",
      "Epoch 19/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3828 - accuracy: 0.0206\n",
      "Epoch 20/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3784 - accuracy: 0.0206\n",
      "Epoch 21/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3802 - accuracy: 0.0206\n",
      "Epoch 22/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3826 - accuracy: 0.0206\n",
      "Epoch 23/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3746 - accuracy: 0.0206\n",
      "Epoch 24/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3782 - accuracy: 0.0206\n",
      "Epoch 25/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3803 - accuracy: 0.0206\n",
      "Epoch 26/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3749 - accuracy: 0.0206\n",
      "Epoch 27/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3785 - accuracy: 0.0206\n",
      "Epoch 28/70\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 9.3717 - accuracy: 0.0206\n",
      "Epoch 29/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3754 - accuracy: 0.0206\n",
      "Epoch 30/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3760 - accuracy: 0.0206\n",
      "Epoch 31/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3750 - accuracy: 0.0206\n",
      "Epoch 32/70\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 9.3734 - accuracy: 0.0206\n",
      "Epoch 33/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3715 - accuracy: 0.0206\n",
      "Epoch 34/70\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 9.3776 - accuracy: 0.0206\n",
      "Epoch 35/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3692 - accuracy: 0.0206\n",
      "Epoch 36/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3702 - accuracy: 0.0206\n",
      "Epoch 37/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3697 - accuracy: 0.0206\n",
      "Epoch 38/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3704 - accuracy: 0.0206\n",
      "Epoch 39/70\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 9.3734 - accuracy: 0.0206\n",
      "Epoch 40/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3701 - accuracy: 0.0206\n",
      "Epoch 41/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3699 - accuracy: 0.0206\n",
      "Epoch 42/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3769 - accuracy: 0.0206\n",
      "Epoch 43/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3734 - accuracy: 0.0206\n",
      "Epoch 44/70\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 9.3688 - accuracy: 0.0206\n",
      "Epoch 45/70\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 9.3695 - accuracy: 0.0206\n",
      "Epoch 46/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3696 - accuracy: 0.0206\n",
      "Epoch 48/70\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 9.3703 - accuracy: 0.0206\n",
      "Epoch 49/70\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 9.3701 - accuracy: 0.0206\n",
      "Epoch 50/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3675 - accuracy: 0.0206\n",
      "Epoch 51/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3694 - accuracy: 0.0206\n",
      "Epoch 52/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3672 - accuracy: 0.0206\n",
      "Epoch 53/70\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 9.3689 - accuracy: 0.0206\n",
      "Epoch 54/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3695 - accuracy: 0.0206\n",
      "Epoch 55/70\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 9.3657 - accuracy: 0.0206\n",
      "Epoch 56/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3659 - accuracy: 0.0206\n",
      "Epoch 57/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3707 - accuracy: 0.0206\n",
      "Epoch 58/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3676 - accuracy: 0.0206\n",
      "Epoch 59/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3677 - accuracy: 0.0206\n",
      "Epoch 60/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3682 - accuracy: 0.0206\n",
      "Epoch 61/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3701 - accuracy: 0.0206\n",
      "Epoch 62/70\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 9.3668 - accuracy: 0.0206\n",
      "Epoch 63/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3651 - accuracy: 0.0206\n",
      "Epoch 64/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3662 - accuracy: 0.0206\n",
      "Epoch 65/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3641 - accuracy: 0.0206\n",
      "Epoch 66/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3679 - accuracy: 0.0206\n",
      "Epoch 67/70\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 9.3643 - accuracy: 0.0206\n",
      "Epoch 68/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3663 - accuracy: 0.0206\n",
      "Epoch 69/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3690 - accuracy: 0.0206\n",
      "Epoch 70/70\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 9.3664 - accuracy: 0.0206\n"
     ]
    }
   ],
   "source": [
    "#Create LSTM end to end model (ie directly predicts fantasy points rather than\n",
    "#stats for the 10th game)\n",
    "end_end_model = LSTM_stats_model(10, 12, end_to_end = True)\n",
    "end_end_model.summary()\n",
    "\n",
    "#Transform the y_train (m, 12) array to a (m) array of fantasy scores from each\n",
    "#10th game\n",
    "y_train_scores = scores_from_stats(y_train)\n",
    "#Train model\n",
    "with tf.device('/device:GPU:0'):\n",
    "  history3 = end_end_model.fit(X_train, y_train_scores, epochs=70, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "O26fB4iudTF1",
    "outputId": "714c509d-b083-410d-af0a-57f211d2d1a2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss of the end to end model\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlwDO1tadsMz",
    "outputId": "5f3834bf-a2a3-46eb-fcc2-05a44248df5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: end_end_model_05_27_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: end_end_model_05_27_2/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fe9f1de9890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fe9f1dd7f50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fe9f1cc8710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fe9f1d3d6d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 4s 5ms/step - loss: 9.2360 - accuracy: 0.0215\n",
      "tf.Tensor([1158.5244   251.88367  261.36078 ...  388.33246  262.64703  249.92307], shape=(19835,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Save and Evaluate LSTM end to end model\n",
    "end_end_model.save(\"end_end_model_05_27_2\")\n",
    "y_test_scores = scores_from_stats(y_test)\n",
    "end_end_model.evaluate(X_test, y_test_scores)\n",
    "\n",
    "y_pred = end_end_model.predict(X_test)\n",
    "loss = tf.keras.losses.mean_squared_error(y_test_scores, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9dhsjte5PVP"
   },
   "source": [
    "Flattened, deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hDrtaFEV_Vx"
   },
   "outputs": [],
   "source": [
    "#Flatten the games into a (m, 180) array from a (m, 9, 20) array for predicting \n",
    "#the outcome of the 10th game using a deep neural network\n",
    "#Predict either the stats or directly teh fantasy points of the 10th game\n",
    "def flattened_Deep_Model(Tx, stats_mode = False):\n",
    "  input = tf.keras.Input(shape=((Tx-1), 20))  \n",
    "\n",
    "  #normalize the input and flatten it into a single array\n",
    "  mid = normalize_layer(input)\n",
    "  mid = tf.keras.layers.Flatten()(mid)\n",
    "\n",
    "  #pass the input through 4 dense layers, each with batch normalization, dropout\n",
    "  #and a RELU activation function\n",
    "  mid = tf.keras.layers.Dense(512)(mid)\n",
    "  mid = tf.keras.layers.BatchNormalization()(mid)\n",
    "  mid = tf.keras.activations.relu(mid)\n",
    "  mid = tf.keras.layers.Dropout(0.2)(mid)\n",
    "  mid = tf.keras.layers.Dense(256)(mid)\n",
    "  mid = tf.keras.layers.BatchNormalization()(mid)\n",
    "  mid = tf.keras.activations.relu(mid)\n",
    "  mid = tf.keras.layers.Dropout(0.2)(mid)\n",
    "  mid = tf.keras.layers.Dense(64)(mid)\n",
    "  mid = tf.keras.layers.BatchNormalization()(mid)\n",
    "  mid = tf.keras.activations.relu(mid)\n",
    "  mid = tf.keras.layers.Dropout(0.2)(mid)\n",
    "  mid = tf.keras.layers.Dense(32, activation=\"relu\")(mid)\n",
    "  mid = tf.keras.layers.Dropout(0.2)(mid)\n",
    "  mid = tf.keras.layers.Dense(16, activation=\"relu\")(mid)\n",
    "\n",
    "  #In the final layer, predict the stats of the 10th game\n",
    "  out = tf.keras.layers.Dense(12, activation=\"linear\")(mid)\n",
    "\n",
    "  #optionally add a final layer for directly predicting fantasy scores\n",
    "  if not stats_mode:\n",
    "    out = tf.keras.layers.Dense(1, activation=\"linear\")(mid)\n",
    "\n",
    "  #declare the model\n",
    "  model_flat = tf.keras.Model(inputs=input, outputs=out)\n",
    "\n",
    "  #create an optimizer for the model\n",
    "  opt = tf.keras.optimizers.Adam(lr=0.02, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "  #compile the model\n",
    "  model_flat.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
    "  \n",
    "  return model_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhyyply-8cOV",
    "outputId": "a184eaf3-7290-4538-87fd-380da47424c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 9, 20)]           0         \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 9, 20)            41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 180)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               92672     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " tf.nn.relu (TFOpLambda)     (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_1 (TFOpLambda)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_2 (TFOpLambda)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 246,629\n",
      "Trainable params: 244,924\n",
      "Non-trainable params: 1,705\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create flat, deep stats out model\n",
    "flat_stats_model = flattened_Deep_Model(10, stats_mode = True)\n",
    "flat_stats_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XE94Z5vq0pPs",
    "outputId": "916f60f4-9c7e-4017-f74e-d4af1f33d3ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5908 - accuracy: 0.6059\n",
      "Epoch 2/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5666 - accuracy: 0.6069\n",
      "Epoch 3/150\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.5606 - accuracy: 0.6055\n",
      "Epoch 4/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5604 - accuracy: 0.6055\n",
      "Epoch 5/150\n",
      "5889/5889 [==============================] - 66s 11ms/step - loss: 1.5581 - accuracy: 0.6054\n",
      "Epoch 6/150\n",
      "5889/5889 [==============================] - 65s 11ms/step - loss: 1.5563 - accuracy: 0.6051\n",
      "Epoch 7/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5567 - accuracy: 0.6049\n",
      "Epoch 8/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5555 - accuracy: 0.6042\n",
      "Epoch 9/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5558 - accuracy: 0.6053\n",
      "Epoch 10/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5547 - accuracy: 0.6055\n",
      "Epoch 11/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5539 - accuracy: 0.6066\n",
      "Epoch 12/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5532 - accuracy: 0.6052\n",
      "Epoch 13/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5533 - accuracy: 0.6051\n",
      "Epoch 14/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5530 - accuracy: 0.6065\n",
      "Epoch 15/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5525 - accuracy: 0.6066\n",
      "Epoch 16/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5525 - accuracy: 0.6076\n",
      "Epoch 17/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5522 - accuracy: 0.6080\n",
      "Epoch 18/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5512 - accuracy: 0.6086\n",
      "Epoch 19/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5512 - accuracy: 0.6089\n",
      "Epoch 20/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5519 - accuracy: 0.6081\n",
      "Epoch 21/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5509 - accuracy: 0.6098\n",
      "Epoch 22/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5511 - accuracy: 0.6101\n",
      "Epoch 23/150\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.5512 - accuracy: 0.6102\n",
      "Epoch 24/150\n",
      "5889/5889 [==============================] - 56s 9ms/step - loss: 1.5504 - accuracy: 0.6103\n",
      "Epoch 25/150\n",
      "5889/5889 [==============================] - 56s 10ms/step - loss: 1.5504 - accuracy: 0.6111\n",
      "Epoch 26/150\n",
      "5889/5889 [==============================] - 56s 9ms/step - loss: 1.5513 - accuracy: 0.6113\n",
      "Epoch 27/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5506 - accuracy: 0.6108\n",
      "Epoch 28/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5506 - accuracy: 0.6107\n",
      "Epoch 29/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5502 - accuracy: 0.6110\n",
      "Epoch 30/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5497 - accuracy: 0.6112\n",
      "Epoch 31/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5500 - accuracy: 0.6115\n",
      "Epoch 32/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5501 - accuracy: 0.6111\n",
      "Epoch 33/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5501 - accuracy: 0.6115\n",
      "Epoch 34/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5499 - accuracy: 0.6116\n",
      "Epoch 35/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5494 - accuracy: 0.6130\n",
      "Epoch 36/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5492 - accuracy: 0.6127\n",
      "Epoch 37/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5492 - accuracy: 0.6111\n",
      "Epoch 38/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5491 - accuracy: 0.6116\n",
      "Epoch 39/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5494 - accuracy: 0.6115\n",
      "Epoch 40/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5489 - accuracy: 0.6117\n",
      "Epoch 41/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5496 - accuracy: 0.6118\n",
      "Epoch 42/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5486 - accuracy: 0.6115\n",
      "Epoch 43/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5494 - accuracy: 0.6123\n",
      "Epoch 44/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5490 - accuracy: 0.6121\n",
      "Epoch 45/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5485 - accuracy: 0.6114\n",
      "Epoch 46/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5482 - accuracy: 0.6124\n",
      "Epoch 47/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5482 - accuracy: 0.6126\n",
      "Epoch 48/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5489 - accuracy: 0.6120\n",
      "Epoch 49/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5482 - accuracy: 0.6124\n",
      "Epoch 50/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5484 - accuracy: 0.6121\n",
      "Epoch 51/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5488 - accuracy: 0.6121\n",
      "Epoch 52/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5479 - accuracy: 0.6121\n",
      "Epoch 53/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5476 - accuracy: 0.6119\n",
      "Epoch 54/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5481 - accuracy: 0.6114\n",
      "Epoch 55/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5488 - accuracy: 0.6116\n",
      "Epoch 56/150\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.5480 - accuracy: 0.6103\n",
      "Epoch 57/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5483 - accuracy: 0.6116\n",
      "Epoch 58/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5477 - accuracy: 0.6098\n",
      "Epoch 59/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5474 - accuracy: 0.6109\n",
      "Epoch 60/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5478 - accuracy: 0.6100\n",
      "Epoch 61/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5484 - accuracy: 0.6103\n",
      "Epoch 62/150\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.5474 - accuracy: 0.6097\n",
      "Epoch 63/150\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.5486 - accuracy: 0.6094\n",
      "Epoch 64/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5474 - accuracy: 0.6089\n",
      "Epoch 65/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5474 - accuracy: 0.6091\n",
      "Epoch 66/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5485 - accuracy: 0.6088\n",
      "Epoch 67/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5476 - accuracy: 0.6103\n",
      "Epoch 68/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5474 - accuracy: 0.6097\n",
      "Epoch 69/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5473 - accuracy: 0.6094\n",
      "Epoch 70/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5472 - accuracy: 0.6083\n",
      "Epoch 71/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5472 - accuracy: 0.6087\n",
      "Epoch 72/150\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.5472 - accuracy: 0.6084\n",
      "Epoch 73/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5481 - accuracy: 0.6086\n",
      "Epoch 74/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5475 - accuracy: 0.6083\n",
      "Epoch 75/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5474 - accuracy: 0.6067\n",
      "Epoch 76/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5475 - accuracy: 0.6070\n",
      "Epoch 77/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5477 - accuracy: 0.6071\n",
      "Epoch 78/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5478 - accuracy: 0.6071\n",
      "Epoch 79/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5477 - accuracy: 0.6067\n",
      "Epoch 80/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5469 - accuracy: 0.6068\n",
      "Epoch 81/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5470 - accuracy: 0.6067\n",
      "Epoch 82/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5472 - accuracy: 0.6073\n",
      "Epoch 83/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5465 - accuracy: 0.6079\n",
      "Epoch 84/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5463 - accuracy: 0.6069\n",
      "Epoch 85/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5467 - accuracy: 0.6066\n",
      "Epoch 86/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5466 - accuracy: 0.6065\n",
      "Epoch 87/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5469 - accuracy: 0.6070\n",
      "Epoch 88/150\n",
      "5889/5889 [==============================] - 65s 11ms/step - loss: 1.5465 - accuracy: 0.6063\n",
      "Epoch 89/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5476 - accuracy: 0.6060\n",
      "Epoch 90/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5467 - accuracy: 0.6057\n",
      "Epoch 91/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5466 - accuracy: 0.6061\n",
      "Epoch 92/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5471 - accuracy: 0.6057\n",
      "Epoch 93/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5471 - accuracy: 0.6060\n",
      "Epoch 94/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5466 - accuracy: 0.6059\n",
      "Epoch 95/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5462 - accuracy: 0.6059\n",
      "Epoch 96/150\n",
      "5889/5889 [==============================] - 64s 11ms/step - loss: 1.5470 - accuracy: 0.6056\n",
      "Epoch 97/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5464 - accuracy: 0.6060\n",
      "Epoch 98/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5470 - accuracy: 0.6055\n",
      "Epoch 99/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5469 - accuracy: 0.6049\n",
      "Epoch 100/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5471 - accuracy: 0.6044\n",
      "Epoch 101/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5463 - accuracy: 0.6052\n",
      "Epoch 102/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5458 - accuracy: 0.6054\n",
      "Epoch 103/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5460 - accuracy: 0.6051\n",
      "Epoch 104/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5465 - accuracy: 0.6054\n",
      "Epoch 105/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5466 - accuracy: 0.6053\n",
      "Epoch 106/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5464 - accuracy: 0.6054\n",
      "Epoch 107/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5459 - accuracy: 0.6052\n",
      "Epoch 108/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5464 - accuracy: 0.6047\n",
      "Epoch 109/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5464 - accuracy: 0.6042\n",
      "Epoch 110/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5460 - accuracy: 0.6045\n",
      "Epoch 111/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5466 - accuracy: 0.6037\n",
      "Epoch 112/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5458 - accuracy: 0.6048\n",
      "Epoch 113/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5464 - accuracy: 0.6050\n",
      "Epoch 114/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5467 - accuracy: 0.6044\n",
      "Epoch 115/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5460 - accuracy: 0.6046\n",
      "Epoch 116/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5455 - accuracy: 0.6047\n",
      "Epoch 117/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5459 - accuracy: 0.6049\n",
      "Epoch 118/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5452 - accuracy: 0.6046\n",
      "Epoch 119/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5454 - accuracy: 0.6042\n",
      "Epoch 120/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5461 - accuracy: 0.6043\n",
      "Epoch 121/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5461 - accuracy: 0.6051\n",
      "Epoch 122/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5460 - accuracy: 0.6048\n",
      "Epoch 123/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5450 - accuracy: 0.6047\n",
      "Epoch 124/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5463 - accuracy: 0.6056\n",
      "Epoch 125/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5458 - accuracy: 0.6062\n",
      "Epoch 126/150\n",
      "5889/5889 [==============================] - 63s 11ms/step - loss: 1.5457 - accuracy: 0.6054\n",
      "Epoch 127/150\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.5452 - accuracy: 0.6054\n",
      "Epoch 128/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5461 - accuracy: 0.6056\n",
      "Epoch 129/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5456 - accuracy: 0.6052\n",
      "Epoch 130/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5459 - accuracy: 0.6060\n",
      "Epoch 131/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5466 - accuracy: 0.6048\n",
      "Epoch 132/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5465 - accuracy: 0.6043\n",
      "Epoch 133/150\n",
      "5889/5889 [==============================] - 57s 10ms/step - loss: 1.5461 - accuracy: 0.6039\n",
      "Epoch 134/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5459 - accuracy: 0.6036\n",
      "Epoch 135/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5450 - accuracy: 0.6045\n",
      "Epoch 136/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5455 - accuracy: 0.6043\n",
      "Epoch 137/150\n",
      "5889/5889 [==============================] - 58s 10ms/step - loss: 1.5454 - accuracy: 0.6040\n",
      "Epoch 138/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5452 - accuracy: 0.6044\n",
      "Epoch 139/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5454 - accuracy: 0.6050\n",
      "Epoch 140/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5456 - accuracy: 0.6046\n",
      "Epoch 141/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5455 - accuracy: 0.6039\n",
      "Epoch 142/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5464 - accuracy: 0.6038\n",
      "Epoch 143/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5452 - accuracy: 0.6043\n",
      "Epoch 144/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5458 - accuracy: 0.6043\n",
      "Epoch 145/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5451 - accuracy: 0.6037\n",
      "Epoch 146/150\n",
      "5889/5889 [==============================] - 59s 10ms/step - loss: 1.5454 - accuracy: 0.6040\n",
      "Epoch 147/150\n",
      "5889/5889 [==============================] - 61s 10ms/step - loss: 1.5446 - accuracy: 0.6038\n",
      "Epoch 148/150\n",
      "5889/5889 [==============================] - 62s 10ms/step - loss: 1.5452 - accuracy: 0.6040\n",
      "Epoch 149/150\n",
      "5889/5889 [==============================] - 62s 11ms/step - loss: 1.5454 - accuracy: 0.6037\n",
      "Epoch 150/150\n",
      "5889/5889 [==============================] - 60s 10ms/step - loss: 1.5452 - accuracy: 0.6038\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "with tf.device('/device:GPU:0'):\n",
    "  history2 = flat_stats_model.fit(X_train, y_train, epochs=150, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "-px9VaaKpxew",
    "outputId": "dc48fd40-2d94-47f5-fd89-461df73a99e7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss of the training\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzsxIVLS8q61",
    "outputId": "c30f43de-9554-48dd-ebad-27b9732d2bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: flat_stats_out_model_05_27/assets\n",
      "620/620 [==============================] - 2s 3ms/step - loss: 1.5442 - accuracy: 0.5953\n",
      "[5.96771768 2.35372414 3.82578922 1.00611616 2.13416311 2.59772203\n",
      " 1.33511313 2.2250705  1.88879934 1.04187216 0.91243362 1.26703663]\n"
     ]
    }
   ],
   "source": [
    "#Save and evaluate flat, deep stats out model\n",
    "flat_stats_model.save(\"flat_stats_out_model_05_27\")\n",
    "flat_stats_model.evaluate(X_test, y_test)\n",
    "\n",
    "y_pred = flat_stats_model.predict(X_test)\n",
    "loss = stat_specific_loss(y_test, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zX3fCCOu9GkO",
    "outputId": "188e5c8a-e2f1-46f3-e0b5-ee1ed241533e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(188.080880262514, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#Evaluate stats out model in terms of fantasy points\n",
    "y_pred = flat_stats_model.predict(X_test)\n",
    "y_pred_scores = scores_from_stats(y_pred)\n",
    "y_test_scores = scores_from_stats(y_test)\n",
    "loss = tf.keras.losses.mean_squared_error(y_test_scores, y_pred_scores)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oke0iW96jKEs",
    "outputId": "0bdc8f72-4ca8-46a9-dfe4-2a5e43af9011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 2s 3ms/step - loss: 6.4170 - accuracy: 0.6492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.41702938079834, 0.6492059230804443]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model2 = tf.keras.models.load_model(\"flat_stats_out_model_05_27\")\n",
    "loaded_model2.evaluate(normalized_X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pip22rb99HBo"
   },
   "source": [
    "Load and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgDNrmmEbbW4",
    "outputId": "61b6234e-25e9-4b5f-cfef-669a3ac90844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/freshman year/CS230/Notebooks/Final Project\n"
     ]
    }
   ],
   "source": [
    "#Navigate to relevant directory\n",
    "%cd /content/gdrive/My Drive/freshman year/CS230/Notebooks/Final Project\n",
    "\n",
    "#Declare relevant methods also outlined above\n",
    "def scores_from_stats(stats):\n",
    "  fantasy_score_arr = np.array([1,2,-1,1,1,-1,1,1,2,4,4,-2])\n",
    "  score = np.zeros((stats.shape[0]))\n",
    "  for i in range(stats.shape[0]):\n",
    "    score[i] = np.dot(stats[i], fantasy_score_arr)\n",
    "  score = np.squeeze(score)\n",
    "  return score\n",
    "\n",
    "def stat_specific_loss(y_true, y_hat):\n",
    "  #shape of y_true and y_hat is (m, 12), calculates RMS\n",
    "  loss = np.sqrt((1/y_true.shape[0]) * np.sum((y_true - y_hat) ** 2, axis = 0))\n",
    "  return loss\n",
    "\n",
    "#An array used to convert game stats to a fantasy score\n",
    "fantasy_score_arr = np.array([0,1,0,0,0,0,2,-1,1,0,1,-1,1,1,2,4,4,-2,0,0])\n",
    "\n",
    "#load in a test set and evaluate the models on that test set\n",
    "test_set = np.load(\"test_set.npy\")\n",
    "X_test = np.zeros((test_set.shape[0], 9, 20))\n",
    "y_test = np.zeros((test_set.shape[0]))\n",
    "for i in range(test_set.shape[0]):\n",
    "  X_test[i] = test_set[i].T[0:9, :]\n",
    "  y_test[i] = np.squeeze(np.dot(test_set[i].T[9, :], fantasy_score_arr))\n",
    "\n",
    "\n",
    "\n",
    "#load LSTM models\n",
    "loaded_LSTM_model = tf.keras.models.load_model(\"stats_out_model_05_27\")\n",
    "loaded_LSTM_end_model = tf.keras.models.load_model(\"end_end_model_05_27_2\")\n",
    "\n",
    "#load deep, flat model\n",
    "loaded_flat_model = tf.keras.models.load_model(\"flat_stats_out_model_05_27\")\n",
    "\n",
    "#both models take in (m, 9, 20) shaped numpy array X_test vectors\n",
    "\n",
    "#create predictions for stats out LSTM model\n",
    "y_pred1 = loaded_LSTM_model.predict(X_test)\n",
    "y_pred_scores1 = scores_from_stats(y_pred1)\n",
    "\n",
    "#create predictions for stats out, flat and deep NN\n",
    "y_pred2 = loaded_flat_model.predict(X_test)\n",
    "y_pred_scores2 = scores_from_stats(y_pred2)\n",
    "\n",
    "#create predictions for end to end LSTM model\n",
    "y_pred_3 = loaded_LSTM_end_model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88D6nT1H0riN"
   },
   "outputs": [],
   "source": [
    "#define predictions and actual_vals using one of the three model predictions\n",
    "predictions = y_pred_3\n",
    "actual_vals = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPRpidxu0nHa"
   },
   "outputs": [],
   "source": [
    "num_test_examples = predictions.shape[0]\n",
    "# Compute arrays representing l1 and l2 (MSE) losses per test example\n",
    "l1_losses = np.abs(predictions - actual_vals)\n",
    "l2_losses = np.square(predictions - actual_vals)\n",
    "    \n",
    "# Get total for l1 and l2 losses\n",
    "total_l1_loss = np.sum(l1_losses)\n",
    "total_l2_loss = np.sum(l2_losses)\n",
    " \n",
    "# Get the average l1 and l2 loss per test example\n",
    "avg_l1_loss = total_l1_loss/num_test_examples\n",
    "avg_l2_loss = total_l2_loss/num_test_examples\n",
    " \n",
    "# Get the median l1 and l2 losses over all test examples\n",
    "median_l1_loss = np.median(np.abs(predictions - actual_vals))\n",
    "median_l2_loss = np.median(np.square(predictions - actual_vals))\n",
    " \n",
    "# Print out the found values above\n",
    "print(\"Average l1 loss: \" + str(avg_l1_loss))\n",
    "print(\"Median l1 loss: \" + str(median_l1_loss))\n",
    "print(\"Average l2 loss: \" + str(avg_l2_loss))\n",
    "print(\"Median l2 loss: \" + str(median_l2_loss))\n",
    " \n",
    "# Graph histogram for l1 losses by percent in each category\n",
    "plt.clf()\n",
    "plt.hist(l1_losses, 10, weights=np.ones(len(l1_losses)) / len(l1_losses))\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.title(\"Histogram with l1 losses (abs fantasy point prediction errors)\")\n",
    "plt.show()\n",
    " \n",
    "# Graph histogram for l2 losses by percent in each category\n",
    "plt.clf()\n",
    "plt.hist(l2_losses, 10, weights=np.ones(len(l2_losses)) / len(l2_losses))\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.title(\"Histogram with l2 losses (squared fantasy point prediction errors)\")\n",
    "plt.show()\n",
    " \n",
    "# Graph histogram for l1 losses by percent in each category\n",
    "plt.clf()\n",
    "x = np.linspace(0,70,100)\n",
    "y = 1*x+0\n",
    "plt.plot(x, y, '-r', label='y=x (Perfect prediction line)')\n",
    "plt.scatter(predictions, actual_vals, alpha = 0.2)\n",
    "plt.title(\"Scatter Plot of Predictions on x-axis and corresponding actual values on y-axis\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS230 Notebook Commented.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
